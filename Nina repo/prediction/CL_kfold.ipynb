{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-29T23:37:59.415392100Z",
     "start_time": "2023-11-29T23:37:52.963528500Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../Nina repo')\n",
    "import scipy.optimize._minimize\n",
    "import torchvision\n",
    "from dataloader.dataloader import DISEASE_LABELS_NIH,NIHDataResampleModule, DISEASE_LABELS_CHE, CheXpertDataResampleModule\n",
    "from prediction.models import ResNet,DenseNet\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from misc import *\n",
    "from gmm_and_mix_class import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female_percent_in_training_set:[50]\n",
      "disease_label_list:['Pneumothorax']\n"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "#parser.add_argument('--gpus', default=1)\n",
    "parser.add_argument('--dev', default=0)\n",
    "\n",
    "#dataset_default = \"chexpert\"\n",
    "#img_dir_default = r\"C:\\Users\\Karlu\\Desktop\\11\\Learning From Noisy Data\\archivepreproc_224x224\"\n",
    "\n",
    "dataset_default = \"NIH\"\n",
    "img_dir_default = r\"C:\\Users\\Karlu\\Desktop\\11\\Learning From Noisy Data\\NIH\\preproc_224x224\"\n",
    "\n",
    "disease_label_default = ['Pneumothorax']\n",
    "female_percent_in_training_default = \"50\"\n",
    "npp_default = 1 #Number per patient, could be integer or None (no sampling)\n",
    "run_dir_default = r\"C:\\Users\\Karlu\\Desktop\\11\\Learning From Noisy Data\\runs\"\n",
    "\n",
    "epochs_default = 38\n",
    "pretrained_default = True\n",
    "save_model_default = False\n",
    "num_workers_default = 0\n",
    "augmentation_default = True\n",
    "model_scale_default = '50'\n",
    "batch_size_default = 85\n",
    "lr_default = 1e-4\n",
    "\n",
    "# hps that need to chose when training\n",
    "parser.add_argument('-s','--dataset',default=dataset_default,help='Dataset', choices =['NIH','chexpert'])\n",
    "parser.add_argument('-d','--disease_label',default=disease_label_default, help='Chosen disease label', type=str, nargs='*')\n",
    "parser.add_argument('-f', '--female_percent_in_training', default=female_percent_in_training_default,\n",
    "                    help='Female percentage in training set, should be any of [0, 50, 100]', type=str, nargs='+')\n",
    "parser.add_argument('-n', '--npp',default=npp_default, help='Number per patient, could be integer or None (no sampling)',type=int)\n",
    "parser.add_argument('-r', '--random_state', default='0-10', help='random state')\n",
    "parser.add_argument('-p','--img_dir', default=img_dir_default, help='your img dir path here',type=str)\n",
    "parser.add_argument('-rd','--run_dir', default=run_dir_default, help='where the runs are saved',type=str)\n",
    "\n",
    "# hps that set as defaults\n",
    "parser.add_argument('--lr', default=lr_default, help='learning rate, default=1e-6')\n",
    "parser.add_argument('--bs', default=batch_size_default, help='batch size, default=64')\n",
    "parser.add_argument('--epochs',default=epochs_default,help='number of epochs, default=20')\n",
    "parser.add_argument('--model', default='resnet', help='model, default=\\'ResNet\\'')\n",
    "parser.add_argument('--model_scale', default=model_scale_default, help='model scale, default=50',type=str)\n",
    "parser.add_argument('--pretrained', default=pretrained_default, help='pretrained or not, True or False, default=True',type=lambda x: (str(x).lower() == 'true'))\n",
    "parser.add_argument('--augmentation', default=augmentation_default, help='augmentation during training or not, True or False, default=True',type=lambda x: (str(x).lower() == 'true'))\n",
    "parser.add_argument('--is_multilabel',default=False,help='training with multilabel or not, default=False, single label training',type=lambda x: (str(x).lower() == 'true'))\n",
    "parser.add_argument('--image_size', default=224,help='image size',type=int)\n",
    "parser.add_argument('--crop',default=None,help='crop the bottom part of the image, the percentage of cropped part, when cropping, default=0.6')\n",
    "parser.add_argument('--prevalence_setting',default='separate',help='which kind of prevalence are being used when spliting,\\\n",
    "                    choose from [separate, equal, total]',choices=['separate','equal','total'])\n",
    "parser.add_argument('--save_model',default=save_model_default,help='dave model parameter or not',type=lambda x: (str(x).lower() == 'true'))\n",
    "parser.add_argument('--num_workers', default=num_workers_default, help='number of workers')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# other hps\n",
    "if args.is_multilabel:\n",
    "    args.num_classes = len(DISEASE_LABELS_NIH) if args.dataset == 'NIH' else len(DISEASE_LABELS_CHE)\n",
    "else: args.num_classes = 1\n",
    "\n",
    "\n",
    "if args.image_size == 224:\n",
    "    args.img_data_dir = args.img_dir+'{}/preproc_224x224/'.format(args.dataset)\n",
    "elif args.image_size == 1024:\n",
    "    args.img_data_dir = args.img_dir+'{}/images/'.format(args.dataset)\n",
    "\n",
    "if args.dataset == 'NIH':\n",
    "    args.csv_file_img = '../datafiles/'+'Data_Entry_2017_v2020_clean_split.csv'\n",
    "elif args.dataset == 'chexpert':\n",
    "    args.csv_file_img = '../datafiles/'+'chexpert.sample.allrace.csv'\n",
    "else:\n",
    "    raise Exception('Not implemented.')\n",
    "\n",
    "#print('hyper-parameters:')\n",
    "#print(args)\n",
    "\n",
    "if len(args.random_state.split('-')) != 2:\n",
    "    if len(args.random_state.split('-')) == 1:\n",
    "        rs_min, rs_max = int(args.random_state), int(args.random_state)+1\n",
    "    else:\n",
    "        raise Exception('Something wrong with args.random_states : {}'.format(args.random_states))\n",
    "rs_min, rs_max = int(args.random_state.split('-')[0]),int(args.random_state.split('-')[1])\n",
    "\n",
    "# female_percent_in_training_set = [int(percent) for percent in args.female_percent_in_training.split(\" \")]\n",
    "female_percent_in_training_set = [50]\n",
    "print('female_percent_in_training_set:{}'.format(female_percent_in_training_set))\n",
    "disease_label_list = args.disease_label #[''.join(each) for each in args.disease_label]\n",
    "if len(disease_label_list) ==1 and disease_label_list[0] == 'all':\n",
    "    disease_label_list = DISEASE_LABELS_NIH if args.dataset == 'NIH' else DISEASE_LABELS_CHE\n",
    "print('disease_label_list:{}'.format(disease_label_list))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T23:37:59.477874Z",
     "start_time": "2023-11-29T23:37:59.431011100Z"
    }
   },
   "id": "c9823c7767dac834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE:cuda:0\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "\n",
      "run_config: NIH-Pneumothorax-fp50-npp1-rs0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Karlu\\Desktop\\11\\Learning From Noisy Data\\Nina repo\\dataloader\\dataloader.py:389: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_per_patient = df.groupby(['Patient ID', 'Patient Gender']).mean()\n",
      "C:\\Users\\Karlu\\Desktop\\11\\Learning From Noisy Data\\Nina repo\\dataloader\\dataloader.py:418: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_per_patient = df.groupby([self.col_name_patient_id, self.col_name_gender]).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists and is loaded0\n",
      "['Pneumothorax']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 8458/8458 [00:00<00:00, 45178.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pneumothorax']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 1409/1409 [00:00<00:00, 45137.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pneumothorax']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Data: 100%|██████████| 8459/8459 [00:00<00:00, 49218.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train:  8458\n",
      "#val:    1409\n",
      "#test:   8459\n",
      "using mixup approch/method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 1/38: 100%|██████████| 80/80 [00:55<00:00,  1.44batches/s, male val acc=0.5, female val acc=0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1:  avg val pred class: 0.0000, avg train pred class: 0.0034, loss_fn.class_imb_scaling: 4.75 curr lr: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 2/38: 100%|██████████| 80/80 [00:53<00:00,  1.50batches/s, male val acc=0.959, female val acc=0.945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E2:  avg val pred class: 0.0014, avg train pred class: 0.0001, loss_fn.class_imb_scaling: 5.0 curr lr: 0.0000750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 3/38: 100%|██████████| 80/80 [00:53<00:00,  1.48batches/s, male val acc=0.96, female val acc=0.946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E3:  avg val pred class: 0.0035, avg train pred class: 0.0059, loss_fn.class_imb_scaling: 5.25 curr lr: 0.0000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 4/38: 100%|██████████| 80/80 [00:51<00:00,  1.56batches/s, male val acc=0.962, female val acc=0.946]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E4:  avg val pred class: 0.0163, avg train pred class: 0.0126, loss_fn.class_imb_scaling: 5.5 curr lr: 0.0001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 5/38: 100%|██████████| 80/80 [00:52<00:00,  1.53batches/s, male val acc=0.957, female val acc=0.939]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E5:  avg val pred class: 0.0717, avg train pred class: 0.0494, loss_fn.class_imb_scaling: 5.25 curr lr: 0.0000970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 6/38: 100%|██████████| 80/80 [00:52<00:00,  1.52batches/s, male val acc=0.916, female val acc=0.901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E6:  avg val pred class: 0.0774, avg train pred class: 0.0522, loss_fn.class_imb_scaling: 5.0 curr lr: 0.0000883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 7/38: 100%|██████████| 80/80 [00:53<00:00,  1.50batches/s, male val acc=0.926, female val acc=0.891]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E7:  avg val pred class: 0.0781, avg train pred class: 0.0575, loss_fn.class_imb_scaling: 4.75 curr lr: 0.0000750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 8/38:  18%|█▊        | 14/80 [00:09<00:43,  1.52batches/s, male val acc=0.918, female val acc=0.904]"
     ]
    }
   ],
   "source": [
    "def main(args, female_perc_in_training=None, random_state=None, chose_disease_str=None):\n",
    "\n",
    "    # sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "    #pl.seed_everything(42, workers=True)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:\" + str(args.dev) if use_cuda else \"cpu\")\n",
    "    print('DEVICE:{}'.format(device))\n",
    "\n",
    "    # get run_config\n",
    "    run_config = f'{args.dataset}-{chose_disease_str}' # dataset and the predicted label\n",
    "    run_config+= f'-fp{female_perc_in_training}-npp{args.npp}-rs{random_state}' #f_per, npp and rs\n",
    "\n",
    "    # if the hp value is not default\n",
    "    # args_dict = vars(args)\n",
    "    # for each_hp in hp_default_value.keys():\n",
    "    #     if (hp_default_value[each_hp] != args_dict[each_hp] and\n",
    "    #             each_hp!=\"num_workers\"):\n",
    "    #\n",
    "    #         run_config+= f'-{each_hp}{args_dict[each_hp]}'\n",
    "\n",
    "    print('------------------------------------------\\n'*3)\n",
    "    print('run_config: {}'.format(run_config))\n",
    "\n",
    "    # Create output directory\n",
    "    # out_name = str(model.model_name)\n",
    "    run_dir = args.run_dir#'/work3/ninwe/run/cause_bias/'\n",
    "    out_dir = run_dir + run_config\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    cur_version = get_cur_version(out_dir)\n",
    "\n",
    "    if args.dataset == 'NIH':\n",
    "        data = NIHDataResampleModule(img_data_dir=args.img_data_dir,\n",
    "                                     csv_file_img=args.csv_file_img,\n",
    "                                     image_size=args.image_size,\n",
    "                                     pseudo_rgb=False,\n",
    "                                     batch_size=args.bs, #90 is limit i.e. 10.9gb vram\n",
    "                                     num_workers=args.num_workers,\n",
    "                                     augmentation=args.augmentation,\n",
    "                                     outdir=out_dir,\n",
    "                                     version_no=cur_version,\n",
    "                                     female_perc_in_training=female_perc_in_training,\n",
    "                                     chose_disease=chose_disease_str,\n",
    "                                     random_state=random_state,\n",
    "                                     num_classes=args.num_classes,\n",
    "                                     num_per_patient=args.npp,\n",
    "                                     crop=args.crop,\n",
    "                                     prevalence_setting = args.prevalence_setting,\n",
    "\n",
    "                                     )\n",
    "    elif args.dataset == 'chexpert':\n",
    "        if args.crop != None:\n",
    "            raise Exception('Crop experiment not implemented for chexpert.')\n",
    "        data = CheXpertDataResampleModule(img_data_dir=args.img_data_dir,\n",
    "                                          csv_file_img=args.csv_file_img,\n",
    "                                          image_size=args.image_size,\n",
    "                                          pseudo_rgb=False,\n",
    "                                          batch_size=args.bs, #90 is limit i.e. 10.9gb vram\n",
    "                                          num_workers=args.num_workers,\n",
    "                                          augmentation=args.augmentation,\n",
    "                                          outdir=out_dir,\n",
    "                                          version_no=cur_version,\n",
    "                                          female_perc_in_training=female_perc_in_training,\n",
    "                                          chose_disease=chose_disease_str,\n",
    "                                          random_state=random_state,\n",
    "                                          num_classes=args.num_classes,\n",
    "                                          num_per_patient=args.npp,\n",
    "                                          prevalence_setting = args.prevalence_setting\n",
    "\n",
    "                                          )\n",
    "\n",
    "    else:\n",
    "        raise Exception('not implemented')\n",
    "    \n",
    "    \n",
    "    from sklearn.model_selection import KFold\n",
    "    k_folds=5\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "\n",
    "  # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_ids, test_ids) in enumerate(kfold.split(data.train_set)):\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "        \n",
    "        path = os.path.join(os.getcwd(), \"kfold_mixup_exp\", f\"fold{fold}\")\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        \n",
    "        data_loader = data.train_dataloader(subsampler=train_subsampler)\n",
    "        data_loader_holdout = data.train_dataloader(subsampler=test_subsampler)\n",
    "        # model\n",
    "        if args.model == 'resnet':\n",
    "            model_type = ResNet\n",
    "        elif args.model == 'densenet':\n",
    "            model_type = DenseNet\n",
    "        model = model_type(num_classes=args.num_classes,lr=args.lr,pretrained=args.pretrained,model_scale=args.model_scale,\n",
    "                           loss_func_type = 'BCE')\n",
    "    \n",
    "        batch_size = args.bs\n",
    "        model.to(device)\n",
    "        epochs = args.epochs\n",
    "        lr = args.lr\n",
    "        class_imbalance_train = data.df_train[args.disease_label[0]].values.mean()\n",
    "        #class_imbalance_train = torch.tensor([class_imbalance_train], device=device, dtype=torch.float)\n",
    "        class_imbalance_init = 4.5\n",
    "        #loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=class_imbalance)\n",
    "        loss_fn = bce_with_logits_mannual(class_imb=class_imbalance_init, n=len(data.df_train))\n",
    "        optimizer = torch.optim.AdamW(params=model.model.parameters(), lr=lr)\n",
    "        #optimizer = torch.optim.AdamW(params=model.model.parameters(), lr=lr)\n",
    "        #scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=50)\n",
    "        scheduler = scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=3)\n",
    "    \n",
    "        losses = []\n",
    "        accs = []\n",
    "        lrs = [lr]\n",
    "        accu_female_ALL = [0.5]\n",
    "        accu_male_ALL = [0.5]\n",
    "        auroc_male_ALL = [0.5]\n",
    "        auroc_female_ALL = [0.5]\n",
    "        batch_num = [0]\n",
    "        #AUROC(task='binary', num_labels=num_classes, average='macro', thresholds=None)\n",
    "    \n",
    "        if args.dataset == \"NIH\":\n",
    "            is_female_val = data.df_valid[\"Patient Gender\"].values == \"F\"\n",
    "            is_female_val = torch.from_numpy(is_female_val).to(torch.bool).to(device)\n",
    "            is_female_train = data.df_train[\"Patient Gender\"].values == \"F\"\n",
    "            #is_female_train = torch.from_numpy(is_female_train).to(torch.bool).to(device)\n",
    "        else:\n",
    "            is_female_val = data.df_valid[\"sex\"].values == \"Female\"\n",
    "            is_female_val = torch.from_numpy(is_female_val).to(torch.bool).to(device)\n",
    "            is_female_train = data.df_train[\"sex\"].values == \"Female\"\n",
    "            #is_female_train = torch.from_numpy(is_female_train).to(torch.bool).to(device)\n",
    "    \n",
    "        beta = torch.distributions.beta.Beta(20.0, 20.0)\n",
    "        method = \"mixup\"\n",
    "        print(f\"using {method} approch/method\")\n",
    "        assert method in [\"plain\", \"gmm and mixup\", \"mixup\", \"gmm\"]\n",
    "        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            prog_bar = tqdm(data_loader, unit=\"batches\")\n",
    "            prog_bar.set_description(f\"train epoch {epoch+1}/{epochs}\")\n",
    "            prog_bar.set_postfix({\"male val acc\":accu_male_ALL[-1], \"female val acc\": accu_female_ALL[-1] })\n",
    "    \n",
    "            losses_non_reduced = []\n",
    "            ids = []\n",
    "            predicted_class_train = []\n",
    "            ys = []\n",
    "            for i, xy in enumerate(prog_bar):\n",
    "                x = xy[\"image\"].to(device)\n",
    "                y = xy[\"label\"].to(device)\n",
    "                id = xy[\"id\"].to(device)\n",
    "                ys.append(y)\n",
    "                ids.append(id)\n",
    "    \n",
    "                if method == \"plain\":\n",
    "                    logits_pure = model.forward(x)\n",
    "                    loss = loss_fn.bce_with_logits_plain(logits = logits_pure, y=y, reduce=True)\n",
    "    \n",
    "                elif method == \"gmm and mixup\":\n",
    "                    x_mix, y_mix, delta, perm = mixup(x, y, beta, device)\n",
    "                    logit_mixup = model.forward(x_mix)\n",
    "    \n",
    "                    with torch.no_grad():\n",
    "                        logits_pure = model.forward(x)\n",
    "                        loss_pure = loss_fn.bce_with_logits_plain(logit_mixup, y)\n",
    "    \n",
    "                    loss1 = loss_fn.bce_mix_up_gmm(logit_mixup, loss_pure, y_mix, perm=None, epoch=epoch)\n",
    "                    loss2 = loss_fn.bce_mix_up_gmm(logit_mixup, loss_pure, y_mix, perm=perm, epoch=epoch)\n",
    "                    loss = 0.5*(loss1 + loss2)\n",
    "    \n",
    "                elif method == \"mixup\":\n",
    "                    x_mix, y_mix, delta, perm = mixup(x, y, beta, device)\n",
    "                    logit_mixup = model.forward(x_mix)\n",
    "                    loss = loss_fn.bce_with_logits_plain(logit_mixup, y_mix, s=loss_fn.class_imb_scaling, reduce=True)\n",
    "                    with torch.no_grad():\n",
    "                        logits_pure = model.forward(x)\n",
    "    \n",
    "                elif method == \"gmm\":\n",
    "                    logits_pure = model.forward(x)\n",
    "                    with torch.no_grad():\n",
    "                        loss_pure = loss_fn.bce_with_logits_plain(logits_pure, y)\n",
    "                    loss = loss_fn.bce_mix_up_gmm(logits_pure, loss_pure, y, perm=None, epoch=epoch)\n",
    "    \n",
    "                else:\n",
    "                    AssertionError(\"pick a proper model!\")\n",
    "    \n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                logits_pure = logits_pure.detach()\n",
    "                losses_non_reduced.append(loss_fn.bce_with_logits_plain(logits_pure, y))\n",
    "                prediction = torch.sigmoid(logits_pure)\n",
    "                predicted_class_train.append(prediction > 0.5)\n",
    "                correct_prediction = ((prediction > 0.5) == y).to(torch.float)\n",
    "                accs.append(torch.mean(correct_prediction))\n",
    "    \n",
    "                losses.append(loss.detach())\n",
    "    \n",
    "            predicted_class_train = torch.concat(predicted_class_train, dim=0)\n",
    "            avg_predicted_class_train = predicted_class_train.to(torch.float).mean().item()\n",
    "            loss_fn.class_imb_scaling_step(0.25, avg_predicted_class_train, class_imbalance_train)\n",
    "            #\n",
    "    \n",
    "            losses_non_reduced = torch.concat(losses_non_reduced)\n",
    "            ids = torch.concat(ids)\n",
    "            if \"gmm\" in method:\n",
    "                loss_fn.fit_and_plot_gmm(epoch_num=epoch,\n",
    "                                    losses_non_reduced=losses_non_reduced,\n",
    "                                    y=torch.concat(ys).cpu().numpy().squeeze(),\n",
    "                                    preds=predicted_class_train.to(torch.int).cpu().numpy().squeeze(),\n",
    "                                    is_female=is_female_train,\n",
    "                                    ids = ids,\n",
    "                                    plot_hist=True)\n",
    "    \n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "            scheduler.step()\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                val_predictions = []\n",
    "                val_labels = []\n",
    "    \n",
    "                for xy in data.val_dataloader():\n",
    "                #for xy in tqdm(data.val_dataloader(), desc=\"val \"):\n",
    "                    x = xy[\"image\"]\n",
    "                    y = xy[\"label\"]\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    forward_logit = model.forward(x)\n",
    "                    prediction = torch.sigmoid(forward_logit)\n",
    "    \n",
    "                    val_predictions.append(prediction)\n",
    "                    val_labels.append(y)\n",
    "    \n",
    "                print(f\"E{epoch+1}: \"\n",
    "                      f\" avg val pred class:\"\n",
    "                      f\" {torch.mean((torch.concat(val_predictions, dim=0) > 0.5).to(torch.float)).item():.4f},\"\n",
    "                      f\" avg train pred class:\"\n",
    "                      f\" {avg_predicted_class_train:.4f},\"\n",
    "                      f\" loss_fn.class_imb_scaling: {loss_fn.class_imb_scaling}\"\n",
    "                      f\" curr lr: {scheduler.get_last_lr()[0]:.7f}\")\n",
    "    \n",
    "                predictions_female = torch.concat(val_predictions, dim=0)[is_female_val]\n",
    "                labels_female = torch.concat(val_labels, dim=0)[is_female_val]\n",
    "                accu_female = model.accu_func(predictions_female, labels_female).item()\n",
    "                auroc_female = model.auroc_func(predictions_female, labels_female).item()\n",
    "                accu_female_ALL.append(accu_female)\n",
    "                auroc_female_ALL.append(auroc_female)\n",
    "    \n",
    "                predictions_male = torch.concat(val_predictions, dim=0)[~is_female_val]\n",
    "                labels_male = torch.concat(val_labels, dim=0)[~is_female_val]\n",
    "                accu_male = model.accu_func(predictions_male, labels_male).item()\n",
    "                auroc_male = model.auroc_func(predictions_male, labels_male).item()\n",
    "                accu_male_ALL.append(accu_male)\n",
    "                auroc_male_ALL.append(auroc_male)\n",
    "    \n",
    "                batch_num.append((epoch+1)*len(data_loader))\n",
    "    \n",
    "            #prog_bar.set_postfix({\"cur lr\": scheduler.get_last_lr()})\n",
    "            plt.figure(figsize=[7, 7])\n",
    "            plt.plot(ewma(torch.stack(losses).cpu().numpy().astype(np.float64), 50), label=\"TRAIN loss (smooth)\")\n",
    "            plt.plot(ewma(torch.stack(accs).cpu().numpy().astype(np.float64), 50), label =\"TRAIN accu (smooth)\")\n",
    "            plt.plot(batch_num, accu_male_ALL, marker='o', ls=\"--\", label =f\"val acc male: {accu_male_ALL[-1]:.3f}\")\n",
    "            plt.plot(batch_num, accu_female_ALL, marker='o', label =f\"val acc female: {accu_female_ALL[-1]:.3f}\")\n",
    "    \n",
    "            plt.plot(batch_num, auroc_male_ALL, marker='*', ls=\"--\", label =f\"val AUROC male: {auroc_male_ALL[-1]:.3f}\")\n",
    "            plt.plot(batch_num, auroc_female_ALL, marker='*', label =f\"val AUROC female: {auroc_female_ALL[-1]:.3f}\")\n",
    "            plt.plot(batch_num, np.asarray(lrs) / np.asarray(lrs).max(), label= \"LR (normalized)\",\n",
    "                 marker='o', alpha=0.5)\n",
    "            plt.title(method + \" resnet\"+args.model_scale +\" \"+ args.dataset +\"  epoch: \"+ str(epoch+1))\n",
    "            batch_num_array = np.linspace(0, batch_num[-1], 5, dtype=int)\n",
    "            plt.xticks(batch_num_array, batch_num_array*batch_size)\n",
    "            plt.xlabel(\"training samples\")\n",
    "            #plt.ylabel(\"loss and accu\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(path, \"plot.png\"), dpi=150)\n",
    "            #plt.show()\n",
    "           \n",
    "        print(f\"fold {fold} has been completed saving data\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            forward_logits = []\n",
    "            ys = []\n",
    "            ids = []\n",
    "            for xy in data_loader_holdout:\n",
    "                x = xy[\"image\"]\n",
    "                y = xy[\"label\"]\n",
    "                id = xy[\"id\"]\n",
    "                x = x.to(device)\n",
    "                forward_logit = model.forward(x)\n",
    "                \n",
    "                forward_logits.append(forward_logit)\n",
    "                ys.append(y)\n",
    "                ids.append(id)\n",
    "              \n",
    "        is_fem = torch.from_numpy(is_female_train[torch.cat(ids, dim = 0).cpu()])   \n",
    "        data_dict = {\n",
    "            \"forward_logits\": torch.cat(forward_logits, dim = 0).cpu().tolist(),\n",
    "            \"ys\": torch.cat(ys, dim = 0).cpu().tolist(),\n",
    "            \"ids\": torch.cat(ids, dim = 0).cpu().tolist(),\n",
    "            \"is_fem\": is_fem.tolist()\n",
    "        }\n",
    "\n",
    "        plt.show()\n",
    "        torch.save(torch.cat(forward_logits, dim = 0).cpu(), os.path.join(path, \"logits.pt\"))\n",
    "        torch.save(torch.cat(ys, dim = 0).cpu(), os.path.join(path, \"ys.pt\"))\n",
    "        torch.save(torch.cat(ids, dim = 0).cpu(), os.path.join(path, \"ids.pt\"))\n",
    "        torch.save(is_fem, os.path.join(path, \"is_fem.pt\"))\n",
    "        \n",
    "        \n",
    "        import json     \n",
    "        json.dump(data_dict, open(os.path.join(path, \"meta.json\"), 'w'))\n",
    "        torch.save(model, os.path.join(path, \"model.pt\"))\n",
    "        \n",
    "\n",
    "\n",
    "#shutdown -s -f -t (14000)\n",
    "for d in disease_label_list:\n",
    "    for female_perc_in_training in female_percent_in_training_set:\n",
    "        for i in np.arange(rs_min, rs_max):\n",
    "            main(args, female_perc_in_training=female_perc_in_training,random_state = i,chose_disease_str=d)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-29T23:37:59.477874Z"
    }
   },
   "id": "95a101382b01020f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9e0614e32055d778"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
